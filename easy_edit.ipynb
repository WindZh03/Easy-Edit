{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "from diffusers import DDIMScheduler, DDIMInverseScheduler\n",
    "from pipeline_stable_diffusion_grounded_instruct_pix2pix import StableDiffusionInstructPix2PixPipeline\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import os \n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75916d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pil_image(image_path, resolution=512):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    width, height = image.size\n",
    "    factor = resolution / max(width, height)\n",
    "    factor = math.ceil(min(width, height) * factor / 64) * 64 / min(width, height)\n",
    "    width = int((width * factor) // 64) * 64\n",
    "    height = int((height * factor) // 64) * 64\n",
    "    image = ImageOps.fit(image, (width, height), method=Image.Resampling.LANCZOS)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2729e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask extractor\n",
    "device = 'cuda:0'\n",
    "\n",
    "# pipeline\n",
    "num_timesteps = 100\n",
    "pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\"timbrooks/instruct-pix2pix\",\n",
    "                                                                  torch_dtype=torch.float16,\n",
    "                                                                  safety_checker=None).to(device)\n",
    "pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config, set_alpha_to_zero=False)\n",
    "\n",
    "pipeline.scheduler.set_timesteps(num_timesteps)\n",
    "pipeline.inverse_scheduler.set_timesteps(num_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e642f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(pipeline, image_pil, instruction, \n",
    "              image_guidance_scale, text_guidance_scale, seed, blending_range):\n",
    "    external_mask_pil, chosen_noun_phrase = mask_extractor.get_external_mask(image_pil, instruction, verbose=verbose)\n",
    "\n",
    "    inv_results = pipeline.invert(instruction, image_pil, num_inference_steps=num_timesteps, inv_range=blending_range)\n",
    "\n",
    "    generator = torch.Generator(device).manual_seed(seed) if seed is not None else torch.Generator(device)\n",
    "    edited_image = pipeline(instruction, src_mask=external_mask_pil, image=image_pil,\n",
    "                            guidance_scale=text_guidance_scale, image_guidance_scale=image_guidance_scale,\n",
    "                            num_inference_steps=num_timesteps, generator=generator).images[0]\n",
    "    return edited_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from external_mask_extractor import ExternalMaskExtractor  \n",
    "\n",
    "mask_extractor = ExternalMaskExtractor(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default values of Instruct-Pix2Pix are a good starting point,\n",
    "# but you can often get better results with higher guidance_scale\n",
    "\n",
    "##! use our method\n",
    "\n",
    "verbose = True\n",
    "image_path = './test_img/test1.jpg' \n",
    "\n",
    "\n",
    "# edit_instruction = 'turn apples into watermelon'  \n",
    "# edit_instruction = 'turn the third apple into watermelon'   # position\n",
    "# edit_instruction = 'turn the red apple into watermelon'   # color\n",
    "# edit_instruction = 'turn the second largest apple into watermelon'   # size\n",
    "# edit_instruction = 'turn the second red apple into watermelon'   # color + position\n",
    "edit_instruction = 'turn the second largest red apple into egg'   # color + size\n",
    "\n",
    "\n",
    "image_guidance_scale = 1.5\n",
    "guidance_scale = 7.5  \n",
    "# here, steps are defined w.r.t. num_train_steps(=1000)\n",
    "start_blending_at_tstep = 100\n",
    "end_blending_at_tstep   = 1\n",
    "blending_range = [start_blending_at_tstep, end_blending_at_tstep]\n",
    "seed = 42\n",
    "\n",
    "\n",
    "image = load_pil_image(image_path)\n",
    "image.show()\n",
    "\n",
    "edited_image = inference(pipeline, image, edit_instruction, image_guidance_scale, \n",
    "                         guidance_scale, seed, blending_range)\n",
    "print(type(edited_image))\n",
    "edited_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49967029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default values of Instruct-Pix2Pix are a good starting point,\n",
    "# but you can often get better results with higher guidance_scale\n",
    "\n",
    "##! use Instruct-Pix2Pix model\n",
    "\n",
    "image_guidance_scale = 1.5\n",
    "guidance_scale = 7.5  \n",
    "edit_instruction = 'make this picture look like a sketch'   # color + size\n",
    "\n",
    "image.show()\n",
    "edited_image = inference(pipeline, image, edit_instruction, image_guidance_scale, \n",
    "                         guidance_scale, seed, blending_range)\n",
    "edited_image.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
